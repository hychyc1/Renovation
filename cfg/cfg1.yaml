# reward
reward_specs:
  monetary_weight: 0.5
  transportation_weight: 0.5
  POI_weight: 0.5

# agent
agent_specs:
  batch_stage: false

gamma: 0.995
tau: 0.0

# base info
plot_ratio: 0.8
POI_plot_ratio: 0.75
speed_c: 0
speed_r: 0
max_yr: 10
inflation_rate: 0.015
POI_affect_range: 5
space_per_person: 30
occupation_rate: 0.9
FAR_values: [1.8, 1.9, 2.0, 2.1, 2.2]
combinations: [
  [0.4, 0.4, 0.4],
  [0.3, 0.5, 0.2],
  [0.1, 0.6, 0.3],
  [0.2, 0.6, 0.2],
  [0.1, 0.7, 0.2],
  [0.2, 0.7, 0.1],
  [0.4, 0.3, 0.3],
  [0.5, 0.2, 0.3],
  [0.6, 0.2, 0.2],
  [0.6, 0.1, 0.3],
  [0.6, 0.3, 0.1],
  [0.7, 0.2, 0.1],
  [0.3, 0.3, 0.4],
  [0.4, 0.2, 0.4],
  [0.2, 0.4, 0.4]
]


state_encoder_specs:
  grid_attributes: ['AREA', 'pop', 'road', 'price_c', 'price_r', 'POI', 'r_b']
  grid_feature_name: 6
  feature_dim: 

policy_specs:

value_specs:
  value_head_hidden_size: [32, 32, 1]
policy_lr: 4.0e-4
value_lr: 4.0e-4
weightdecay: 0.0
eps: 1.0e-5

value_pred_coef: 0.5
entropy_coef: 0.01
clip_epsilon: 0.2


max_num_iterations: 100
#num_episodes_per_iteration: 500
num_episodes_per_iteration: 1200
#max_sequence_length: 100
max_sequence_length: 37
num_optim_epoch: 4
#mini_batch_size: 1024
mini_batch_size: 1024
save_model_interval: 10


